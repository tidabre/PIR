{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Schema: ['content', 'path', 'title']>\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the directory \"indexdir\" created beforehand in the folder where you start the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = create_in(\"indexdir\", schema)\n",
    "writer = index.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_document(title=u\"First document\", path=u\"/a\",content=u\"This is the first document we've added!\")\n",
    "writer.add_document(title=u\"Second document\", path=u\"/b\",content=u\"The second one is even more interesting!\")\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the query we will use now is \"first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'path': '/a', 'title': 'First document'}> 1.047619047619048\n"
     ]
    }
   ],
   "source": [
    "with index.searcher() as searcher:\n",
    "    query = QueryParser(\"content\", index.schema).parse(\"first\")\n",
    "    results = searcher.search(query)\n",
    "    print(results[0], results[0].score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ranking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh import scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = scoring.TF_IDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'path': '/a', 'title': 'First document'}> 1.0\n"
     ]
    }
   ],
   "source": [
    "with index.searcher(weighting =w) as searcher:\n",
    "    query = QueryParser(\"content\", index.schema).parse(\"first\")\n",
    "    results = searcher.search(query)\n",
    "    print(results[0], results[0].score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define a custom scoring function too. pos_score_fn computes a score for a given document using only one field. Here the score is based on the first occurence (position) of the query term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_score_fn(searcher, fieldname, text, matcher):\n",
    "    poses = matcher.value_as(\"positions\")\n",
    "    return 1.0 / (poses[0] + 1)\n",
    "\n",
    "pos_weighting = scoring.FunctionWeighting(pos_score_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'path': '/a', 'title': 'First document'}> 0.45\n"
     ]
    }
   ],
   "source": [
    "with index.searcher(weighting =pos_weighting) as searcher:\n",
    "    query = QueryParser(\"content\", index.schema).parse(\"first document\")\n",
    "    results = searcher.search(query)\n",
    "    print(results[0], results[0].score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing a collection and computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicode_csv_reader(utf8_data, dialect=csv.excel, **kwargs):\n",
    "    csv_reader = csv.reader(utf8_data, dialect=dialect, **kwargs)\n",
    "    for row in csv_reader:\n",
    "        yield [unicode(cell, 'utf-8') for cell in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(file_path, delimiter='\\t'):\n",
    "    with open(file_path, 'r', encoding='utf8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        doc_list = []\n",
    "        for row in reader:\n",
    "            doc_list.append((row[0],row[1], row[2].replace('\\n',' ')))\n",
    "\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_list = read_file(\"collection.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#docs:  4154\n"
     ]
    }
   ],
   "source": [
    "print('#docs: ',len(doc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our own tf-idf-function: (4.a.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_fn(searcher, fieldname, text, matcher):\n",
    "    tf = matcher.value_as(\"frequency\")\n",
    "    idf = searcher.idf(fieldname, text)\n",
    "    return tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_weighting = scoring.FunctionWeighting(tf_idf_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define 'another ranking function that scores each document as the\n",
    "sum of term frequency multiplied by term position' (4.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_tp_fn(searcher, fieldname, text, matcher):\n",
    "    tf = matcher.value_as(\"frequency\")\n",
    "    tp = matcher.value_as(\"positions\")[0]\n",
    "    return tf*tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_tp_weighting = scoring.FunctionWeighting(tf_tp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = Schema(id=ID(stored=True), content=TEXT)\n",
    "index = create_in(\"cw_index\", schema)\n",
    "writer = index.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in doc_list:\n",
    "    writer.add_document(id=doc[0],content=doc[2])\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read QRels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_qrels(file_path, delimiter=' '):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        qrels = {}\n",
    "        for row in reader:\n",
    "            qrels[(row[0].replace(\"_\", \" \"),row[1])] = int(row[2])\n",
    "\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qrels_hash_map = read_qrels(\"q5.web.qrels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def precision(doc_list, query, qrels, k=10):\n",
    "    f = lambda x: qrels[(x,query)] if (x,query) in qrels else 0\n",
    "    vals = list(map(lambda q: 1 if q>0 else 0, map(f, doc_list[:k])))\n",
    "    return sum(vals)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_relevance(doc_list, query, qrels):\n",
    "    f = lambda x: qrels[(x,query)] if (x,query) in qrels else 0\n",
    "    vals = list(map(f, doc_list))\n",
    "    print(vals)\n",
    "    return sum(vals)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def precision_at_10(doc_list, query, qrels):\n",
    "    return precision(doc_list, query, qrels, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate total number of relevant docs for a given query\n",
    "def total_relevant(query, qrels):\n",
    "    return sum(1 if key[0]==query and value>0 else 0 for key,value in qrels_hash_map.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(doc_list, query, qrels):\n",
    "    f = lambda x: qrels[(x,query)] if (x,query) in qrels else 0\n",
    "    vals = list(map(lambda q: 1 if q>0 else 0, map(f, doc_list)))\n",
    "    return sum(vals)/total_relevant(query, qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ndcg_at_10(doc_list, query, qrels):\n",
    "    total_eval = 0\n",
    "    for i in range(10):\n",
    "        gain = qrels[(doc_list[i], query)] if (doc_list[i], query) in qrels else 0\n",
    "        if i>0:\n",
    "            total_eval += gain * 1.0/math.log2(i+1)\n",
    "        else:\n",
    "            total_eval += gain\n",
    "            \n",
    "    return total_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_at_10(doc_list, query, qrels):\n",
    "    total_eval = 0\n",
    "    relevant_count = 0\n",
    "    for i in range(10):\n",
    "        if (qrels[(doc_list[i], query)] if (doc_list[i], query) in qrels else 0) > 0:\n",
    "            relevant_count += 1\n",
    "            total_eval += precision(doc_list, query, qrels, k=i+1)\n",
    "    \n",
    "    if relevant_count > 0:\n",
    "        return total_eval/relevant_count\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_evaluation(doc_list, query, qrels):\n",
    "    print (\"precision@10: \", precision_at_10(doc_list, query, qrels))\n",
    "    print (\"recall: \", recall(doc_list, query, qrels))\n",
    "    print (\"NDCG@10: \", ndcg_at_10(doc_list, query, qrels))\n",
    "    print (\"MAP@10: \", map_at_10(doc_list, query, qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf weighting:\n",
      "Results found: 535\n"
     ]
    }
   ],
   "source": [
    "print(\"tf-idf weighting:\")\n",
    "result_list = []\n",
    "with index.searcher(weighting = tf_idf_weighting) as searcher:\n",
    "    query = QueryParser(\"content\", index.schema).parse(query_strings[3])\n",
    "    results = searcher.search(query, limit=None)\n",
    "    print(\"Results found:\", len(results))\n",
    "    for result in results:\n",
    "        result_list.append(result['id'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_strings = [\"obama family tree\", \"french lick resort and casino\",\"getting organized\",\"toilet\",\"mitchell college\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################\n",
      "query:  obama family tree\n",
      "\n",
      "tf-idf weighting:\n",
      "Results found: 85\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "tf-tp weighting:\n",
      "Results found: 85\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "\n",
      "#######################################\n",
      "query:  french lick resort and casino\n",
      "\n",
      "tf-idf weighting:\n",
      "Results found: 83\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "tf-tp weighting:\n",
      "Results found: 83\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "\n",
      "#######################################\n",
      "query:  getting organized\n",
      "\n",
      "tf-idf weighting:\n",
      "Results found: 468\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "tf-tp weighting:\n",
      "Results found: 468\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "\n",
      "#######################################\n",
      "query:  toilet\n",
      "\n",
      "tf-idf weighting:\n",
      "Results found: 535\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "tf-tp weighting:\n",
      "Results found: 535\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "\n",
      "#######################################\n",
      "query:  mitchell college\n",
      "\n",
      "tf-idf weighting:\n",
      "Results found: 318\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n",
      "\n",
      "tf-tp weighting:\n",
      "Results found: 318\n",
      "precision@10:  0.0\n",
      "recall:  0.0\n",
      "NDCG@10:  0.0\n",
      "MAP@10:  0.0\n"
     ]
    }
   ],
   "source": [
    "for query_str in query_strings:\n",
    "    print(\"\\n\\n#######################################\")\n",
    "    print(\"query: \", query_str)\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"tf-idf weighting:\")\n",
    "    result_list = []\n",
    "    with index.searcher(weighting = tf_idf_weighting) as searcher:\n",
    "        query = QueryParser(\"content\", index.schema).parse(query_str)\n",
    "        results = searcher.search(query, limit=None)\n",
    "        print(\"Results found:\", len(results))\n",
    "        for result in results:\n",
    "            result_list.append(result['id'])\n",
    "        \n",
    "        apply_evaluation(result_list, query_str, qrels_hash_map)\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"tf-tp weighting:\")\n",
    "    result_list = []\n",
    "    with index.searcher(weighting =tf_tp_weighting) as searcher:\n",
    "        query = QueryParser(\"content\", index.schema).parse(query_str)\n",
    "        results = searcher.search(query, limit=None)\n",
    "        print(\"Results found:\", len(results))\n",
    "        for result in results:\n",
    "            result_list.append(result['id'])\n",
    "            \n",
    "        apply_evaluation(result_list, query_str, qrels_hash_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
